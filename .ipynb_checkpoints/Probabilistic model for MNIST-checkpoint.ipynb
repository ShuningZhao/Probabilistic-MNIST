{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST handwritten digits recognition using Probabilistic Models\n",
    "\n",
    "** Last update: Febuary 15th 2018 at 10:35pm **\n",
    "\n",
    "** By Shuning Zhao **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Instructions\n",
    "This code aims to build a class-conditional classifer using Gaussian Mixture Models (GMMs). \n",
    "\n",
    "1. For all the machine-learning related code you have two options: (a) use [scikit-learn](http://scikit-learn.org/stable/) and/or (b) write your own code. In particular, for fitting GMMs or building the classifier, you should refrain from using other packages. \n",
    "2. This code uses the package [scikit-learn Gaussian Mixture](http://scikit-learn.org/stable/modules/mixture.html) for GMM. Use standard (non-variational) Expectation-Maximisation updates for parameter estimation. \n",
    "\n",
    "### Main task\n",
    "My tasks is to build a class-conditional classifier for classifying digits using the MNIST dataset. There is a file `mnist_train.npz` that contains images of digits (0-9). \n",
    "- The features `xtrain`, which have been normalized to be between [0,1], are 784 dimensional vectors corresponding to 28 * 28 image intensities. \n",
    "- The targets `ytrain` contain the class label of each example using one-hot-encoding. \n",
    "- In total there are 60,000 examples, each with one label from the 10 different classes. \n",
    "- The original dataset can be found at http://yann.lecun.com/exdb/mnist/ and you can read more about this dataset there. However, this dataset has been processed and shuffled so the training and test data in this exercise do not correspond to the original sets. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refresher\n",
    "Recall that a class-conditional classifier models the joint distribution of features $\\mathbf{x}$ and classes $y$ as $p(\\mathbf{x}, y) = p(y) p(\\mathbf{x} | y)$ and then uses Bayes' rule $p(y | \\mathbf{x}) \\propto  p(y) p(\\mathbf{x} | y)$ to make predictions. In this assignment, you will use a GMM for each of the conditional densities $p(\\mathbf{x} | y)$ and a Categorical distribution for $p(y)$.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages \n",
    "Required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Necessary libraries\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "import sklearn as skl\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Put the graphs where we can see them\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the function below to plot a digit in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_image(array, dim=28):\n",
    "    \"\"\"\n",
    "    Plot array as an image of dimensions dim * dim\n",
    "    \"\"\"\n",
    "    img = array.reshape(dim,dim, order = \"C\")\n",
    "    pl.imshow(img, cmap=pl.cm.gray)\n",
    "    ax = pl.gca();ax.set_yticks([]);ax.set_xticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how you should load your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.load('mnist_train.npz')\n",
    "\n",
    "# training data\n",
    "xtrain = data['xtrain']\n",
    "ytrain = data['ytrain']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here and example of plotting a specific digit and showing its corresponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO4AAADuCAYAAAA+7jsiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAABjVJREFUeJzt3bFrVHsexuE5ywWLiAiCoCi3MdqatCEQW7WwN1YKov+A\njYL5A4yNlUGEdBaSIom1BBQbLUSLpHItYuEGhDRjIWe72+zOd5JJjPNOnqd9z3EG5cO58Lsz07Rt\n2wGy/OtPvwFg94QLgYQLgYQLgYQLgYQLgYQLgYQLgYQLgf7azcVN0/jfrOA3a9u26XeNJy4EEi4E\nEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4E\nEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4E2tWv9XHwTp8+Xe7Ly8vlPjk5We4bGxvlvrKy0nN79OhR\nee/m5ma5MzhPXAgkXAgkXAgkXAgkXAgkXAgkXAjUtG2784ubZucXs2MnTpzouX3+/Lm89+TJk+W+\nm3/f3ep2u+U+Oztb7ktLS/v5dkZG27ZNv2s8cSGQcCGQcCGQcCGQcCGQcCGQcCGQc9wDUJ3Tdjqd\nzsuXL3tu09PT5b0fP34s9/X19XLv93ncV69e9dzevn1b3ru9vV3uU1NT5f7p06dyH1XOcWFECRcC\nCRcCCRcCCRcCCRcC+XrWAzA+Pl7u/Y58KouLi+X++PHjgf/sfhYWFsr98uXL5d7vuIjePHEhkHAh\nkHAhkHAhkHAhkHAhkHAhkHPcA3Ds2LFyb5q+n+LqaW1tbeB79+r27dt/7LUPO09cCCRcCCRcCCRc\nCCRcCCRcCCRcCOQc9wDcvHmz3KuvyN3c3Czvff/+/UDviWyeuBBIuBBIuBBIuBBIuBBIuBBIuBDI\nOe6Qe/HixZ9+CwwhT1wIJFwIJFwIJFwIJFwIJFwIJFwI5Bz3AOzle5Ph//HEhUDChUDChUDChUDC\nhUDChUCOg4ZAdVw0MzNT3tvv61knJiYGeUv/WFhY6LnNzc2V9/b7alkG54kLgYQLgYQLgYQLgYQL\ngYQLgYQLgZrqJx7/5+Km2fnFh8iRI0fKfXV1tdwvXbo08Gv3+8jg9vb2wH92p9PpHD16tOfW7XbL\ne2dnZ8t9aWlpoPc06tq27fs5UE9cCCRcCCRcCCRcCCRcCCRcCCRcCOQcdx+cPXu23L98+fLbXvvN\nmzflfvfu3XIfHx8v98XFxZ7b2NhYee/Gxka5T01NlfvW1la5jyrnuDCihAuBhAuBhAuBhAuBhAuB\nhAuBnOPug+PHj5f7u3fvyr3fWepeXnuvn8c9depUz215ebm8t993Oj9//rzcb926Ve6jyjkujCjh\nQiDhQiDhQiDhQiDhQiDhQiC/j7sPfvz4Ue7z8/Plfv/+/Z7bnTt3ynv3ek7bz7dv33pua2tr5b2T\nk5PlPj09Xe7VGXW/v/NR54kLgYQLgYQLgYQLgYQLgYQLgRwHHYCnT5/uaR9W/T4S2m8/d+5cuVc/\n8ek4CIgjXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAjk87hD4Pz58z23\nBw8elPfeuHFjv9/OgVlZWSn36qthDztPXAgkXAgkXAgkXAgkXAgkXAgkXAjkHHcIXLx4sed2/fr1\n8t5nz56V++vXrwd5S/+4evVqz21mZqa8t2macv/+/Xu5//r1q9wPM09cCCRcCCRcCCRcCCRcCCRc\nCOQ4aAhsbW313H7+/Fnee+3atXLvdrvlfuHChXJ/8uRJz21sbKy8d319vdzv3btX7vTmiQuBhAuB\nhAuBhAuBhAuBhAuBhAuBmrZtd35x0+z8YvbF169fy/3MmTPlvpt/393qd0Y8Pz9f7v2+evawatu2\n/jxkxxMXIgkXAgkXAgkXAgkXAgkXAgkXAjnHHXJXrlwp97m5uXKfmJjY0+t/+PCh5/bw4cPy3tXV\n1T299mHlHBdGlHAhkHAhkHAhkHAhkHAhkHAhkHNcGDLOcWFECRcCCRcCCRcCCRcCCRcCCRcCCRcC\nCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcC\nCRcCCRcCCRcC/bXL6//T6XT+/TveCNDpdDqdv3dy0a5+HxcYDv5TGQIJFwIJFwIJFwIJFwIJFwIJ\nFwIJFwIJFwL9F1rvFqkWhbDJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x161b09fa780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 10\n",
    "plot_image(xtrain[idx,:])\n",
    "print (ytrain[idx,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the code for training your model are using the function `train_model` below. \n",
    "\n",
    "- You should be able to run your notebook (by clicking 'Cell->Run All') without errors. However, you must save the trained model in the file `model.npz`. This file will be loaded to make predictions in section 2 and assess the performance of the model in section 3.\n",
    "\n",
    "- It is strongly discouraged to use a full covariance for each of the components of the Gaussian mixture, as the number of parameters grows quadratically on the dimensionality of the data and you will be unable to fit the file size cap in your submission (besides running into various numerical issues). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(xtrain, ytrain):\n",
    "    \"\"\"\n",
    "    Algorithm:\n",
    "    Step 1: Use PCA for dimensionality reduction. Name the new training set xtrain_pca.\n",
    "    Step 2: Fit the data to multiple GMMs, use BIC for model selection.\n",
    "    Step 3: Predict the latent variables of xtrain_pca with the GMM from previous step.\n",
    "    Step 4: Append the latent variables to xtrain_pca for model training.\n",
    "    Step 5: Decode the ytrain data from one-hot format, so it can be used for training QDA.\n",
    "    Step 6: Fit this dataset to Quadratic Discriminant Analysis, which is a class-conditional classifier models.\n",
    "    Step 7: Save model.\n",
    "    End\n",
    "    \"\"\"\n",
    "    \n",
    "    #Step 1 - PCA Reduction.\n",
    "    pca = PCA(n_components = 50)\n",
    "    pca.fit(xtrain)\n",
    "    xtrain_pca = pca.transform(xtrain)\n",
    "    \n",
    "\n",
    "    #Step 2 - GMM with BIC model selection.\n",
    "    lowest_bic = np.infty\n",
    "    bic = []\n",
    "    n_components_range = range(10,120)\n",
    "    cv_types = ['spherical', 'tied', 'diag', 'full']\n",
    "    for cv_type in cv_types:\n",
    "        for n_components in n_components_range:\n",
    "            gmm = GaussianMixture(n_components=n_components,covariance_type=cv_type)\n",
    "            gmm.fit(xtrain_pca)\n",
    "            bic.append(gmm.bic(xtrain_pca))\n",
    "            if bic[-1] < lowest_bic:\n",
    "                lowest_bic = bic[-1]\n",
    "                best_gmm = gmm\n",
    "    \n",
    "    #Step 3 and 4 - Append the latent variables to our training set.\n",
    "    gmm = best_gmm\n",
    "    z1 = gmm.predict(xtrain_pca)\n",
    "    z1 = np.matrix(z1)\n",
    "    xtrain_pca = np.matrix(xtrain_pca)\n",
    "    xtrain_lat = np.concatenate( (xtrain_pca, np.matrix.transpose(z1)), axis = 1 )            \n",
    "                \n",
    "    #Step 5 - Decoding one-hot for ytrain\n",
    "    ytrain_de = list()\n",
    "    for i in range( len(ytrain)):\n",
    "        ytrain_de.append(np.argmax(ytrain[i]))\n",
    "    \n",
    "    #Step 6 - Train QDA\n",
    "    qda = QuadraticDiscriminantAnalysis()\n",
    "    qda.fit(xtrain_lat, ytrain_de)\n",
    "    \n",
    "    #Step 7 - Save Model\n",
    "    np.savez_compressed(\n",
    "        'model.npz',\n",
    "        gmm=gmm,\n",
    "        pca=pca,\n",
    "        model=qda\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will assume that there is a file `mnist_test.npz` from which we will load the test data. The function `make_predictions` should load the `model.npz` file, which must contain all the data structures necessary for making predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO4AAADuCAYAAAA+7jsiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngYQLgYQLgf4HtbhcnwqiqsIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x161b0553438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = np.load('mnist_test.npz')\n",
    "# test data\n",
    "xtest = test['xtest']\n",
    "ytest = test['ytest']\n",
    "idx = 1000\n",
    "plot_image(xtest[idx,:])\n",
    "print (ytest[idx,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_predictions(xtest):\n",
    "    \"\"\"\n",
    "    @param xtest: (Ntest,D)-array with test data\n",
    "    @return class_pred: (N,C)-array with predicted classes using one-hot-encoding \n",
    "    @return class_logprob: (N,C)-array with  predicted log probability of the classes   \n",
    "    \n",
    "    Algorithm:\n",
    "    Step 1 - Load the model.\n",
    "    Step 2 - PCA on xtest.\n",
    "    Step 3 - Predict the latent variables of xtest_pca.\n",
    "    Step 4 - Append the latent variables to xtest_pca.\n",
    "    Step 5 - Make prediction, calculate the class the probability.\n",
    "    Step 6 - One-Hot Encode the results.\n",
    "    Step 7 - Remove the negative infinity in class_logprob, else predictive_performance function will catch error.\n",
    "    End\n",
    "    \"\"\"\n",
    "    \n",
    "    #Step 1 - Load the model.\n",
    "    qda = np.load('model.npz')['model'].tolist();\n",
    "    pca = np.load('model.npz')['pca'].tolist();\n",
    "    gmm = np.load('model.npz')['gmm'].tolist();\n",
    "    \n",
    "    #Step 2 - PCA on xtest.\n",
    "    xtest_pca = pca.transform(xtest)\n",
    "    \n",
    "    #Step 3 - Predict the latent variables of xtest_pca.\n",
    "    z2 = gmm.predict(xtest_pca)\n",
    "    z2 = np.matrix(z2)\n",
    "    \n",
    "    #Step 4 - Append the latent variables to xtest_pca.\n",
    "    xtest_pca = np.matrix(xtest_pca)\n",
    "    xtest_lat = np.concatenate( (xtest_pca, np.matrix.transpose(z2)), axis = 1 )\n",
    "    \n",
    "    #Step 5 - Make prediction, calculate the class the probability.\n",
    "    class_pred = qda.predict(xtest_lat)\n",
    "    class_logprob = qda.predict_log_proba(xtest_lat)\n",
    "    \n",
    "    #Step 6 - One-Hot Encode the results.\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "    class_pred = lb.transform(class_pred)\n",
    "    \n",
    "    #Step 7 - Remove the negative infinity in class_logprob, else predictive_performance function will catch error.\n",
    "    for i in range( len( class_logprob)):\n",
    "        for j in range( len(class_logprob[i])):\n",
    "            if class_logprob[i][j] == -np.infty:\n",
    "                class_logprob[i][j] = np.nan\n",
    "                \n",
    "    return class_pred, class_logprob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance \n",
    "This section tests the generalisation performance of the code. Used to evaluate the performance of the algorithm on a new test test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictive_performance(xdata, ydata, class_pred, class_logprob):\n",
    "    \"\"\"\n",
    "    @param xdata:  (N,D)-array of features \n",
    "    @param ydata:  (N,C)-array of one-hot-encoded true classes\n",
    "    @class_pred: (N,C)-array of one-hot-encoded predicted classes\n",
    "    @class_logprob: (N,C)-array of predicted class log probabilities \n",
    "    \"\"\"\n",
    "    correct = np.zeros(xdata.shape[0])\n",
    "    ltest = np.zeros(xdata.shape[0])\n",
    "    for i, x in enumerate(xdata):\n",
    "        correct[i] = np.all(ydata[i, :] == class_pred[i,:])\n",
    "        ltest[i] = class_logprob[i, np.argmax(ydata[i,:])]\n",
    "    accuracy = correct.mean()\n",
    "    loglike = np.nanmean(ltest)\n",
    "    return accuracy, loglike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class_pred, class_logprob = make_predictions(xtest)\n",
    "accuracy, loglike = predictive_performance(xtest, ytest, class_pred, class_logprob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test accuracy=0.958535353535\n",
      "Average test likelihood=-1.17423137112\n"
     ]
    }
   ],
   "source": [
    "print('Average test accuracy=' + str(accuracy))\n",
    "print('Average test likelihood=' + str(loglike))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "179px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
